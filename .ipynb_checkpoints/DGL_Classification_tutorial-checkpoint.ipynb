{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph classification with DGL\n",
    "\n",
    "Notebook created by Rui Valente de Almeida, for the ISM PhD Course; FCT NOVA, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the [tutorial on graph classification with DGL](https://docs.dgl.ai/en/latest/tutorials/basics/4_batch.html), by the DGL team. In it, we will use some toy examples of graph and implement some GCN code that will allow us to automatically classify a graph as being an instance of one of the labeled classes.\n",
    "\n",
    "Solutions to problems similar to this can be useful for a variety of fields, like cyber security or social network analysis and there are already several papers in the literature on the subject:\n",
    "* [Ying et al](https://arxiv.org/abs/1806.08804);\n",
    "* [Cangea et al., 2018](https://arxiv.org/abs/1811.01287);\n",
    "* [Knyazev et al., 2018](https://arxiv.org/abs/1811.09595);\n",
    "* [Bianchi et al., 2019](https://arxiv.org/abs/1901.01343); \n",
    "* [Liao et al., 2019](https://arxiv.org/abs/1901.01484);\n",
    "* [Gao et al., 2019](https://openreview.net/forum?id=HJePRoAct7);\n",
    "\n",
    "The idea of this tutorial is to use a synthetically generated dataset (generated using one of DGL's functions) to train the neural network and enable the possibility of classification. We will try to balance the dataset, giving the same number of elements to each one of the 8 classes. The classes will contain elements like the following:\n",
    "![8classes](img/dataset_overview.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "As usual, we import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import MiniGCDDataset\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "We now generate our dataset: 80 samples, each graph with size [10,20]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MiniGCDDataset(80, 10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualisation is very important. We should always have somekind of function that allows us to quickly see a sample of what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_ds_sample(graph_index, ret=False):\n",
    "    g, l = dataset[graph_index]\n",
    "    fig, ax = plt.subplots()\n",
    "    nx.draw(g.to_networkx(), ax=ax)\n",
    "    ax.set_title(f'Class:{l}')\n",
    "    plt.show()\n",
    "    \n",
    "    if ret: return g, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_ds_sample(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "When working in deep learning, it is common and good practice to produce mini-batches of whatever we are trying to classify (images, text, documents, ...) before running them through the network we are training. With euclidean data, this is easy: to form a mini-batch of images, we stack how many images we want to stack on a tensor, and it is just another dimension in a matrix. Non euclidean data (graphs) are a different manner, because of their irregularity.\n",
    "\n",
    "To circumvent this inconvenient, the people at GDL have been very clever: they have noticed that a minibatch of graphs can be viewed as a new graph with many disjoint connected components. This makes batching graphs almost a matter of concatenating the graphs that we want to batch. We now define a function that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    return dgl.batch(graphs, labels), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clever trick has another important consequence. The return from `dgl.batch` is a graph, on which we can call whatever method we can call for any other graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Finally we reach the classification stage. It is performed in two stages: first, the nodes pass their feature information (messages) to their neighbours. This phase is called message passing or convolution (as we have seen in the presentation). Afterwards, we perform the aggregation stage, in which a tensor is created to represent information from nodes and edges (globally). Finally, these representations (which can be called embeddings) can be passed into a classifier that will predict the graph label. The next image depicts all this process.\n",
    "![graph_class](img/graph_classifier.png \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#message passing function\n",
    "msg = fn.copy_src(src='h', out='m')\n",
    "\n",
    "# This function takes an average of neighbour features and updates the original node's features.\n",
    "def reduce(nodes): return {'h': torch.mean(nodes.mailbox['m'], 1)}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, node): return {'h': self.activation(self.linear(node.data['h']))}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "        \n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation and classification\n",
    "\n",
    "In this tutorial, initial node features are their degrees. We run the convolution for two rounds and then we aggregate, averaging all node features for each graph in our mini-batch.\n",
    "DGL handles this task through the `mean_nodes()`function, and then it is just a matter of feeding the embeddings to a classifier network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCN(in_dim, hidden_dim, F.relu),\n",
    "            GCN(hidden_dim, hidden_dim, F.relu)\n",
    "        ])\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "    \n",
    "    def forward(self, g):\n",
    "        h = g.in_degrees().view(-1,1).float()\n",
    "        \n",
    "        for conv in self.layers:\n",
    "            h = conv(g, h)\n",
    "        \n",
    "        g.ndata['h'] = h\n",
    "        return self.classify(dgl.mean_nodes(g, 'h'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train\n",
    "\n",
    "We create a dataset with 400 graphs with 10 to 20 nodes. We split the dataset into training in test (80-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size = 400\n",
    "bs = 32\n",
    "lr = 0.001\n",
    "train = MiniGCDataset(0.8*ds_size, 10, 20)\n",
    "test = MiniGCDataset(0.2*ds_size, 10, 20)\n",
    "data_loader = DataLoader(train, batch_size=bs, shuffle=True, collate_fn=collate)\n",
    "\n",
    "model = Classifier(1, 256, trainset.num_classes)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(80):\n",
    "    epoch_loss = 0\n",
    "    for i, (bg, label) in enumerate(data_loader):\n",
    "        prediction = model(bg)\n",
    "        loss = loss_func(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (i + 1)\n",
    "    print(f'Epoch {epoch}, loss {epoch_loss}')\n",
    "    epoch_losses.append(epoch_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
