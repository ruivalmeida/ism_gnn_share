@article{Duvenaud2015,
abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
archivePrefix = {arXiv},
arxivId = {1509.09292},
author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and G{\'{o}}mez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'{a}}n and Adams, Ryan P.},
eprint = {1509.09292},
file = {:Users/ruialmeida/Desktop/gnn/1509.09292.pdf:pdf},
pages = {1--9},
title = {{Convolutional Networks on Graphs for Learning Molecular Fingerprints}},
url = {http://arxiv.org/abs/1509.09292},
year = {2015}
}
@article{Battaglia2016,
abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
archivePrefix = {arXiv},
arxivId = {1612.00222},
author = {Battaglia, Peter W. and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
eprint = {1612.00222},
file = {:Users/ruialmeida/Desktop/gnn/1612.00222.pdf:pdf},
title = {{Interaction Networks for Learning about Objects, Relations and Physics}},
url = {http://arxiv.org/abs/1612.00222},
year = {2016}
}
@article{Zhou2018,
abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require that a model learns from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on graph convolutional network (GCN) and gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
archivePrefix = {arXiv},
arxivId = {1812.08434},
author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
eprint = {1812.08434},
file = {:Users/ruialmeida/Desktop/gnn/1812.08434.pdf:pdf},
pages = {1--20},
title = {{Graph Neural Networks: A Review of Methods and Applications}},
url = {http://arxiv.org/abs/1812.08434},
year = {2018}
}
@article{Wu2019,
abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories. With a focus on graph convolutional networks, we review alternative architectures that have recently been developed; these learning paradigms include graph attention networks, graph autoencoders, graph generative networks, and graph spatial-temporal networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this fast-growing field.},
archivePrefix = {arXiv},
arxivId = {1901.00596},
author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
eprint = {1901.00596},
file = {:Users/ruialmeida/Desktop/gnn/1901.00596.pdf:pdf},
number = {X},
pages = {1--22},
title = {{A Comprehensive Survey on Graph Neural Networks}},
url = {http://arxiv.org/abs/1901.00596},
volume = {X},
year = {2019}
}
@misc{Gaunt2018,
author = {Gaunt, Alex},
title = {{Graph Neural Networks}},
url = {https://youtu.be/cWIeTMklzNg},
year = {2018}
}
@misc{Ganssle,
author = {Ganssle, Graham},
title = {{Introduction to Graph Neural Networks}}
}
@book{Brondy2008,
author = {Brondy, J.A. and Murty, U. S. R.},
editor = {Axler, S. and Ribet, K. A.},
file = {:Users/ruialmeida/Desktop/gnn/Adrian Bondy, U.S.R. Murty - Graph theory-Springer (2008).pdf:pdf},
isbn = {9781846289699},
publisher = {Springer},
title = {{Graduate Texts in Mathematics: Graph Theory}},
year = {2008}
}
@article{Gori2005,
abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphi-cal data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model.},
author = {Gori, Marco and Monfardini, Gabriele and Scarselli, Franco},
doi = {10.1109/IJCNN.2005.1555942},
file = {:Users/ruialmeida/Desktop/gnn/gorietal.pdf:pdf},
isbn = {0780390482},
journal = {Proceedings of the International Joint Conference on Neural Networks},
number = {January},
pages = {729--734},
title = {{A new model for earning in raph domains}},
volume = {2},
year = {2005}
}
